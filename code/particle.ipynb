{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-09T05:17:48.384396Z",
     "start_time": "2024-12-09T05:17:48.381733Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "GAUSS_WEIGHTS = [0.3, 0.3, 0.4]\n",
    "GAUSS_MEANS = [np.zeros(2), np.array([2.0,0.6]), np.array([-1.4,0.2])]\n",
    "GAUSS_COVS = [np.eye(2) * 3.0, np.eye(2)*2.0, np.eye(2) * 0.1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T05:17:48.391886Z",
     "start_time": "2024-12-09T05:17:48.389829Z"
    }
   },
   "id": "df6912a8664bfd55"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def observation_probability(latent: np.ndarray, observation: np.ndarray) -> float:\n",
    "    \"\"\"Given an observation and corresponding latent state, evaluate the likelihood.\n",
    "\n",
    "    Args:\n",
    "        latent: Latent state at current time step.\n",
    "        observation: Observation at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Likelihood p(observation|latent).\n",
    "    \"\"\"\n",
    "    # evaluate the likelihood of the observed position based on the latent position\n",
    "    evaluation_mean = latent[0]\n",
    "    if observation == 0: \n",
    "        likelihood = 1\n",
    "    else:\n",
    "    #likelihood = stats.gamma.pdf(observation, 0.288, loc=evaluation_mean, scale=6.01)\n",
    "        likelihood = stats.norm.pdf(observation, loc = evaluation_mean, scale = 10)\n",
    "    return likelihood\n",
    "\n",
    "def observation_sample(latent: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Given the latent state, sample a observation.\n",
    "\n",
    "    Args:\n",
    "        latent: Latent state at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Sampled observation.\n",
    "    \"\"\"\n",
    "    # Let's keep things somewhat 'simple' by making our distribution a sum of Gaussians\n",
    "    index = np.random.choice(np.arange(3), p=GAUSS_WEIGHTS)\n",
    "    evaluation_mean = latent + GAUSS_MEANS[index]\n",
    "    cov = GAUSS_COVS[index]\n",
    "    return stats.multivariate_normal(mean=evaluation_mean, cov=cov).rvs()\n",
    "\n",
    "def latent_sample(delta_t, latent: np.ndarray,observation) -> np.ndarray:\n",
    "    \"\"\"Given the latent state, sample the next latent state.\n",
    "\n",
    "    Args:\n",
    "        latent: Latent state at current time step.\n",
    "\n",
    "    Returns:\n",
    "        Sampled latent state.\n",
    "    \"\"\"\n",
    "    # Let's keep things somewhat 'simple' by making our distribution a sum of Gaussians\n",
    "    transition_matrix = np.array([\n",
    "        [1, delta_t, 0.5 * delta_t**2],\n",
    "        [0, 1 + np.random.rand(), delta_t],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    if observation == 0:\n",
    "        sample=latent\n",
    "    else:\n",
    "        sample = transition_matrix @ latent\n",
    "    return sample\n",
    "    \n",
    "\n",
    "def particle_filter(measured:dict, n_samples: int, dim_z) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    t = measured[\"time\"]\n",
    "    observations = measured[\"depth\"]\n",
    "\n",
    "    # Placeholder for all of our samples and weights.\n",
    "    z_samples = np.zeros((len(observations) + 1, n_samples, dim_z))\n",
    "    weights = np.zeros((len(observations) + 1, n_samples))\n",
    "    \n",
    "    # Draw initial samples and set initial weights.\n",
    "    z_samples[0] = np.zeros((n_samples,dim_z))#np.array([np.random.uniform(0,.01, size = n_samples), np.random.uniform(-.5,.5, size = n_samples),np.random.uniform(-.01,.01, size = n_samples) ]).T\n",
    "    weights[0] = np.ones((n_samples))*(1/n_samples) \n",
    "    \n",
    "    # Now let's start our particle filtering loop.\n",
    "    for time in range(1,len(observations)+1):\n",
    "            # Sample from the next latent state given the current latent state.\n",
    "        dt = t[time] - t[time-1]\n",
    "        for samp_i in range(n_samples):\n",
    "            # Pick a sample with probability equal to its weight (resampling)\n",
    "            #print(time, samp_i, weights[time-1])\n",
    "            m = np.random.choice(n_samples,p=weights[time-1])\n",
    "            sample_choice = z_samples[time-1][m]\n",
    "\n",
    "            # Move the selected sample and save it\n",
    "            \n",
    "            z_samples[time, samp_i] = latent_sample(dt, sample_choice, observations[time-1]) \n",
    "            # Compute the weights for each of our new samples.\n",
    "            #print(self.compute_w(observations[time-1], z_samples[time]))\n",
    "            weights[time] =  compute_w(observations[time-1], z_samples[time])\n",
    "        #print(np.sum((z_samples[time] * weights[time,:,np.newaxis]), axis=1)[0])\n",
    "    return z_samples, weights\n",
    "\n",
    "def compute_w(observation_t: np.ndarray, z_samples_t: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    # Placeholder for the weights.\n",
    "    weights_t = np.zeros(len(z_samples_t))\n",
    "   \n",
    "    # Calculate each weight. Don't forget to normalize at the end!\n",
    "    for i in range(len(weights_t)):\n",
    "        z_t_i = z_samples_t[i]\n",
    "       # print((z_t_i, observation_t))\n",
    "        weights_t[i] =  observation_probability(z_t_i, observation_t) \n",
    "    weights_t /= np.sum(weights_t) \n",
    "        \n",
    "    return weights_t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T05:17:48.415207Z",
     "start_time": "2024-12-09T05:17:48.413169Z"
    }
   },
   "id": "a602c80ef1316535"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "flood_df = pd.read_pickle(\"flood_df\")\n",
    "measured = flood_df.iloc[0][\"signal_padded\"]\n",
    "measured_f = {\"time\":measured[\"time\"]}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T05:17:48.480795Z",
     "start_time": "2024-12-09T05:17:48.418399Z"
    }
   },
   "id": "6d9df55d635c1bc7"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m z_samples, weights \u001B[38;5;241m=\u001B[39m particle_filter(measured, \u001B[38;5;241m2000\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m      2\u001B[0m z_mean \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum((z_samples \u001B[38;5;241m*\u001B[39m weights[:,:,np\u001B[38;5;241m.\u001B[39mnewaxis]), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[0;32mIn[27], line 86\u001B[0m, in \u001B[0;36mparticle_filter\u001B[0;34m(measured, n_samples, dim_z)\u001B[0m\n\u001B[1;32m     83\u001B[0m         z_samples[time, samp_i] \u001B[38;5;241m=\u001B[39m latent_sample(dt, sample_choice, observations[time\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]) \n\u001B[1;32m     84\u001B[0m         \u001B[38;5;66;03m# Compute the weights for each of our new samples.\u001B[39;00m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;66;03m#print(self.compute_w(observations[time-1], z_samples[time]))\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m         weights[time] \u001B[38;5;241m=\u001B[39m  compute_w(observations[time\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], z_samples[time])\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m#print(np.sum((z_samples[time] * weights[time,:,np.newaxis]), axis=1)[0])\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m z_samples, weights\n",
      "Cell \u001B[0;32mIn[27], line 99\u001B[0m, in \u001B[0;36mcompute_w\u001B[0;34m(observation_t, z_samples_t)\u001B[0m\n\u001B[1;32m     97\u001B[0m     z_t_i \u001B[38;5;241m=\u001B[39m z_samples_t[i]\n\u001B[1;32m     98\u001B[0m    \u001B[38;5;66;03m# print((z_t_i, observation_t))\u001B[39;00m\n\u001B[0;32m---> 99\u001B[0m     weights_t[i] \u001B[38;5;241m=\u001B[39m  observation_probability(z_t_i, observation_t) \n\u001B[1;32m    100\u001B[0m weights_t \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(weights_t) \n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m weights_t\n",
      "Cell \u001B[0;32mIn[27], line 17\u001B[0m, in \u001B[0;36mobservation_probability\u001B[0;34m(latent, observation)\u001B[0m\n\u001B[1;32m     14\u001B[0m     likelihood \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#likelihood = stats.gamma.pdf(observation, 0.288, loc=evaluation_mean, scale=6.01)\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m     likelihood \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mnorm\u001B[38;5;241m.\u001B[39mpdf(observation, loc \u001B[38;5;241m=\u001B[39m evaluation_mean, scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m likelihood\n",
      "File \u001B[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:1988\u001B[0m, in \u001B[0;36mrv_continuous.pdf\u001B[0;34m(self, x, *args, **kwds)\u001B[0m\n\u001B[1;32m   1986\u001B[0m cond \u001B[38;5;241m=\u001B[39m cond0 \u001B[38;5;241m&\u001B[39m cond1\n\u001B[1;32m   1987\u001B[0m output \u001B[38;5;241m=\u001B[39m zeros(shape(cond), dtyp)\n\u001B[0;32m-> 1988\u001B[0m putmask(output, (\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mcond0)\u001B[38;5;241m+\u001B[39mnp\u001B[38;5;241m.\u001B[39misnan(x), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbadvalue)\n\u001B[1;32m   1989\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39many(cond):\n\u001B[1;32m   1990\u001B[0m     goodargs \u001B[38;5;241m=\u001B[39m argsreduce(cond, \u001B[38;5;241m*\u001B[39m((x,)\u001B[38;5;241m+\u001B[39margs\u001B[38;5;241m+\u001B[39m(scale,)))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "z_samples, weights = particle_filter(measured, 2000, 3)\n",
    "z_mean = np.sum((z_samples * weights[:,:,np.newaxis]), axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T05:24:12.506180Z",
     "start_time": "2024-12-09T05:17:48.482095Z"
    }
   },
   "id": "ddce76bfd4c03a62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(measured['time'],measured['depth'],label=\"measured\")\n",
    "plt.plot(measured['time'],z_mean[1:,0], label =\"filtered\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-09T05:24:12.502691Z"
    }
   },
   "id": "36f56b78edd26fa8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
